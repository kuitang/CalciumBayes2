\documentclass{article}
\usepackage{nips10submit_e,times}
%\documentstyle[nips07submit_09,times]{article}
\usepackage[square,numbers]{natbib}
\usepackage{amsmath, epsfig}
\usepackage{amsfonts}
\usepackage{subfigure}
\usepackage{graphicx}
\usepackage{amsfonts}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{easybmat}
\usepackage{footmisc}
\renewcommand\algorithmiccomment[1]{// \textit{#1}}
%
\newcommand{\ignore}[1]{}
\newcommand{\comment}[1]{}
\DeclareMathOperator*{\argmax}{arg\,max}

\title{Inferring Direct and Indirect Functional Connectivity Between Neurons From Multiple Neural Spike Train Data}

\author{
Ben Shababo \hspace{1cm} Kui Tang \hspace{1 cm}Frank Wood\\
Columbia University, New York, NY 10027, USA \\
\texttt{\{bms2156,kt2384\}@columbia.edu},
%\texttt{pfau@neurotheory.columbia.edu} 
\texttt{\{fwood\}@stat.columbia.edu} 
}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}
\newcommand{\X}{\mathcal{X}}


\nipsfinalcopy

\begin{document}

\maketitle

\begin{abstract}
Our project aims to model the functional connectivity of neural
microcircuits. On this scale, we are concerned with how the activity
of each individual neuron relates to other nearby neurons in the
population. Though several models and methods have been implemented
to infer neural microcircuit connectivity, these fail to capture
unobserved influences on the microcircuit. In this paper, we address
these hidden influences on the microcircuit by developing a model
which takes into account functional connectivity between observed
neurons over more than one time step as well as inputs to each
neuron which are unmediated by the observed neurons. We then test
this model by simulating a large population of neurons, but only
observing a subpopulation which allows us to compare our inferred
indirect connectivity with the known direct connectivity of the
total population. With a better understanding of the functional
patterns of neural activity at the cellular level, we can begin to
decode the building blocks of neural computation.
\end{abstract}

\section{Introduction}
\label{sec:introduction}

\subsection{Problem Description}

As we learn more and more about the workings of the neuron and of
specialized brain regions, the question increasingly becomes, how
do these pieces sum to a whole? How do the patterns of connectivity
give rise to vision, memory, motor function, and so on? Currently,
a broad picture of the circuitry, or graphical connectivity, of the
brain does not exist, but several projects are underway to organize
the solution of this problem \citep{Marcus2011, Bohland2009}. Efforts
to examine connectivity of the brain focus on scales ranging from
brain regions each comprised of hundreds of millions of cells down
to microcircuits of only a few cells. Further, some of these projects
address structural connectivity and others functional connectivity
\citep{KnowlesBarley2011, Jain2010, Ropireddy2011, Chiang2011, bhattacharya2006}.

In this project, we will focus on the functional connectivity of
microcircuits: how firing activity of one neuron influences the
firing of other nearby neurons.  Importantly, functional connectivity
does not always imply anatomical connectivity; it only implies that
some set of neurons fire together in correlation.  These jointly
firing neurons may have a common input or be linked in a chain,
rather than having monosynaptic connection.

\subsection{Background}

Several strategies have already been employed to infer the functional
connectivity of microcircuits from calcium imaging and MEA data
\citep{Gerwinn2010, takahashi2007, aguiar2009}. Of special interest
to us and our approach are two recent Bayesian approaches. In
\citep{patnaik2011}, a pattern-growth algorithm is used to find
frequent patterns of firing activity. These patterns define mutual
information between neurons which they summarize in a dynamic
Bayesian network. While their methodology presents a contribution
to the study of Bayesian networks, one limitation of this work in
inferring the connectivity of microcircuits is that it only discovers
relationships of excitation. In \citep{mishchencko2011}, network
activity is modeled in terms of a collection of coupled hidden
Markov chains, with each chain corresponding to a single neuron in
the network and the coupling between the chains reflecting the
networkâ€™s connectivity matrix.  To make computation feasible they
used a blockwise-Gibbs sampling method and took advantage of the
parallel computing possibilities when implementing their
expectation-maximization algorithm.

\subsection{Unobserved Neurons as Indirect Inputs}

Although the work to date has done much to address the problem of
functional neural connectivity, there are still improvements to be
made to current models. For example, current models do not address
unobserved inputs to the system. This is a significant limitation,
as current experiments typically monitor only a field ot 10 to 25 
neurons \citep{mishchencko2011}, leading to potential loss of data
to the unobserved blanket surrounding the field.

In this paper, we account for these indirect influences on the
observed neurons by extending the model of \citep{mishchencko2011}
so that it captures functional weights between neurons over multiple
time steps, effectively extending the model back in time. Patanaik
et.  al., modelling long-term neuron spiking with Dynamic Bayesian
networks, found higher-order causative chains in which a set of
neurons induce spiking activity in each other through chains, cycles,
and polychronous circuits \citep{patanaik2011}. Their model is
limited by allowing for only excitatory neural influences, and being
constrained to observed neurons.  Our model captures both positive
and negative indirect influences and accounts for effects of neurons
outside of the observed field.

\section{Methods}

\subsection{Formal Model}
We extend and a parametric generative model proposed by
\citep{mishchencko2011} of joint spike trains on $N$ neurons in
discrete time. Mischencko et al. propose a model to infer the
connectivity matrix $W$, where each entry $w_{ij}$ encodes the
influence of neuron $j$ on the subsequent firing of neuron $i$,
given the history of directly from calcium florescence data. Their
model can be decomposed into one part inferring neural spike train
data from florescence imaging, and another part inferring $W$ from
spike train data. We focus on the latter.

We model neural spike trains as a discrete-time hidden Markov model.
Denote by $ n_i(t) $ whether neuron $i$ fired at time $t$. We observe
the firing of each neuron, $n_i(t), i = 1,...,N$, at each discrete
time step, such that $n_i(t) = 1$ when we observe a spike and $n_i(t)
= 0$ when the neuron is silent. We model $n_i(t)$ as a Bernoulli
random variable with parameter $f(J_i(t))$, where
\begin{equation}
\label{J} J_i(t) = b_i + I_i(t) + \sum_{j=1}^{N} w_{ij}h_{ij}(t)
\end{equation}
where $b_i$ is a baseline and $I_i(t)$ accounts for indirect
influences on neuron $i$ from a fixed window of past time steps.
The history term, $h_{ij}$, encodes the influence of neuron $j$ on
neuron $i$ and is only dependent on the firing of $j$ at time
$t-\Delta$ where $\Delta$ is the size of each discrete time step.
From \citep{mishchencko2011}, we model $h_{ij}(t)$ as an autoregressive
function:
\begin{equation}
\label{h} h_{ij}(t) = (1-\Delta/\tau)h_{ij}(t-\Delta)
  + n_j(t-\Delta)+\sigma\sqrt{\Delta}\epsilon
\end{equation}
where $ \tau_{ij}^h $ is the decay time constant, $\sigma$
is the standard deviation of the noise and $\epsilon$ is a
standard normal random variable representing noise.  The parameterization
of $h_{ij}(t)$ by $\Delta$ ensures that the physical parameters of
the model scale with the timestep size, which is determined by
temporal resolution of observed data and computational complexity
of memory.

In this model, we set $\tau = 0.02\text{s}$ to match
experimentally-observed values \citep{mishchencko2011} and we leave
$\sigma$ to be inferred. Although the model supports separate decay
and noise terms for each pair of neurons, we used single global
values for simplicity.

% units: secs
% tau = 0.02
% delta = 0.01

In addition, we model the indirect inputs to $n_i$ by summing the
influences from all neurons, $n_j(t-s\Delta), s=2,...,S$, where $S$
is the temporal limit on indirect influences. These higher order
interactions are incorporated into the summed input to neuron
$n_i(t)$ by adding the term,
\begin{equation}
\label{new_term}
I_i(t)=\displaystyle\sum\limits_{s=2}^S\sum\limits_{j=1}^N \beta_{ijs}n_j(t-s\Delta) + \lambda_{is},
\end{equation}
to the input function $J_i(t)$. Here, $s$ is the number of time
steps back and $\beta_{ijs}$ is the weight of the indirect influence
of $n_j(t-s\Delta)$ on $n_i(t)$. The term $\lambda_{is}$ represents
the indirect influence from unknown sources, i.e., influences not
mediated through any of the $N$ observed neurons, on $n_i(t)$.
Along with inferring the direct connectivity matrix, $W$, we will
also infer the three dimensional indirect connectivity matrix, $B$,
and the $N\times S-2$ matrix of $\lambda_{is}$ terms.

Following \citep{mishchencko2011}, we define the spiking probability as 
\begin{equation} \label{f}
f(J) = P\left(n>0 | n \sim \text{Poiss}(e^J\Delta)\right) = 1 - \exp(-e^J\Delta)
\end{equation}
which completes the setup for the generalized linear model. 

\subsection{Priors}
In \citep{mishchencko2011}, they use two priors on the connectivity
matrix, $W$, a sparseness prior and a prior which imposes "Dale's
Law" which states that a neuron can only exert either an excitatory
or an inhibitory influence on postsynaptic neurons. We will include
the sparseness prior in our model by adding an L1 regularization
term to each $w_{ij}$. However, given that recent research has shown
that neurotransmitter co-release is more common that once anticipated,
we will reexamine the effect of imposing Dale's Law on our model.
We also add L1 regularization to the indirect influences, extending
the sparsity assumption.
in \citep{patnaik2011}.

\section{Inference}
We fit a maximum a posteriori estimate of the model through EM.  For
each neuron $i$, we represent vector of incoming history terms from
time $t$ as $\mathbf{h}_i(t)$, and similarly for the vector of
incoming weights. $H$ denotes the set of all history terms, $N$
denotes the set of all spike trains, and $\theta$ denotes the set
of all parameters,
$\left\{ b_i, w_{ij}, \beta_{ijs} | i,j = 1 \ldots N, s = 1 \ldots S \right\}$
From (\ref{f}), the complete-data log-likelihood is given by
\begin{align*} \label{Q}  
Q(\theta,\theta^{old}) &= E_{P(H|N,\theta^{old})} \left[ \log{P(H,N|\theta)} \right] 
\\                     &= \int{ \log\left[P(H,N|\theta)\right] P(H|N,\theta^{old}) dH }
\\                     &= \sum_{it} \int \log\left[P(\mathbf{h}_i(t),n_i(t)|\theta^{old})\right] P(\mathbf{h}_i(t)|N,b_i,\mathbf{w}) d\mathbf{h}_i(t)
\end{align*}
where the last equality holds since the integrand can be separated
into a sum of terms for each neuron-timestep $(i,t)$.

In the E step, we are given a new $\theta$ and we evaluate
$Q(\theta,\theta^{old})$ with our current value of $\theta$
set to $\theta^{old}$.

In the M step, we solve
\begin{equation}
\label{M} \theta = \argmax_{\theta} \left\{ Q(\theta,\theta^{old}) \right\}
\end{equation}
using standard convex optimization techniques, since the objective $Q$ is
convex in $\theta$.

TODO TODO TODO
In our initial experiments, we were unable to discern meaningful results
in the estimated $\beta$ terms. We thus modified 
To improve results for estimating $\beta$, we 

\subsection{E Step with Sequential Monte Carlo}
Each neuron $i$ is driven by a separate HMM, with shared parameters.
We evaluate $Q$ for each neuron in parallel.  The observed sequence
is $n_i(t)$; the hidden sequence is the $\mathbf{h}_i(t)$. In
addition, due to the cross-neuron history terms, we require the
spike train data of all other neurons in our inference (though we
only infer the hidden variables for one neuron at a time).  (\ref{h}),
the transition probabilities are linear-Gaussian distributions;
however, the emission probabilities (\ref{f}) are not, so we cannot
use a standard Kalman filter \citep{bishop}.  Instead, we adapt a
sequential Monte Carlo approach used in \citep{volgelstein2009,
mishchencko2011} to estimate flouresence and calcium concentrations
to produce a sampled approximation of $Q(\theta,\theta^{old})$. We
use a standard forward particle filter \citep{bishop} and a marginal
backward smoother \citep{doucet2000} to obtain the approximation
distribution to $P(H|N,\theta^{old})$.

We initialize the particle filter by sampling $M$ particles from a
normal distribution centered at 0 with standard deviation
$\sigma\sqrt{\Delta}$ with uniform weights. At each timestep we draw
particles from the \emph{prior proposal}
$P(\mathbf{h}_i^{(m)}(t)) = P(\mathbf{h}_i(t) | \mathbf{h}_i^{(m)}(t - 1))$
which is a just a normal distribution given by (\ref{h}). We update
the particle weights using the recurrence (omitting $\theta$ for brevity)
\begin{equation} \label{pf}
p_f^{(m)}(t) = p_f^{(m)}(t - 1)P(n_i(t) | \mathbf{h}_i(t)) 
\end{equation}

As the particle filter evolves, many particles will have low
likelihoods in the next timeslice, driving many weights to zero.
To prevent degeneracy, we employ stratified resampling when the
effective number of particles
\begin{equation} \label{Neff}
N_{eff} = \left|\mathbf{p}_f(t)\right|^{-1}
\end{equation}
is less than $M/2$, where $\mathbf{p}_f(t)$ denotes a vector of all
particle weights for one timestep \citep{volgelstein2009}. That is,
we draw $M$ new samples from our current set of samples with
replacement with probabilities proportional to their current weights,
and reset weights to uniform.

Once all samples and forward weights $p_f^(m)$ have been computed,
we run the backwards marginal filter recurrences beginning at $t=T$,
when the filtering and smoothing distributions are identical:
\begin{align}
r^{(m,m')}(t, t - 1) &= p_b^{(m)}\frac{p_f^{(m)}P(\mathbf{h}_i^{(m)}(t)|\mathbf{h}_i^{(m')}(t - 1))}{\sum_{m'} p_f^{(m')}(t - 1) P(\mathbf{h}_i^{(m)}(t)|\mathbf{h}_i^{(m')}(t - 1))} \\
p_b^{(m')}(t - 1)    &= \sum_{m=1}^M r^{(m,m')}(t, t - 1)
\end{align}
giving the approximate distribution
\begin{equation} \label{Ph}
P(\mathbf{h}_i(t) | N, \theta^{old}) = \sum_{m'=1}^{M} p_b^{(m')} \delta\left[\mathbf{h}_i(t) - \mathbf{h}_i^{(m')}(t)\right]
\end{equation}
where $\delta$ is the Dirac delta.

\subsection{M Step}
We performed the M step using MATLAB's optimization toolbox with the
objective function (TODO) subject to the following constraints:
\begin{enumerate}
\item TODO
\end{enumerate}

Evaluating $Q$ is expensive as it requires a complete pass over the
data---up to 15,000 timesteps. Analytical derivatives are essential.
Fortunately, they are not difficult to derive. Denote by
$\theta_1, \theta_2$ any scalar parameters. Then
\begin{align*}
\frac{\partial Q}{\partial \theta_1} &= \left(1 - n_i(t)\right) \left(-\exp{\left\{-e^{J_i(t)}\right\}} \Delta \frac{\partial J}{\partial \theta_1}\right) \\
    &+ n_i(t) \frac{\exp{ \left\{ -e^{J_i(t)} \Delta + J_i(t) \right\}} \Delta \frac{\partial J}{\partial_\theta1} }{1 - \exp{\left\{-e^{J_i(t)}\Delta\right\}}} \\
\frac{\partial^2 Q}{\partial \theta_1 \partial \theta_2} / \frac{\partial J}{\partial \theta_1} \frac{\partial J}{\partial \theta_2} &= \exp{\left\{-e^{J_i(t)\Delta} + J_i(t)\right\}}\Delta \\
 &+ \frac{\exp{\left\{-2e^{J_i(t)}\Delta + 2J_i(t)\right\}}\Delta^2}{1 - \exp{\left\{-e^{J_i(t)\Delta}\right\}}} \\
 &+ \exp{\left\{-e^{J_i(t)\Delta} + 2J_i(t)\right\}} \Delta^2 
\end{align*}
We omit the $\dfrac{\partial^2 J}{\partial \theta_1^2}$, since $J$
is linear in each variable, so the second derivatives are zero. To
compute both the gradient and Hessian, parameter, we simply multiply
the general forms above by each one of:
\begin{align}
\frac{\partial J_i(t)}{\partial b_i}         &= 1 \\
\frac{\partial J_i(t)}{\partial w_{ij}}      &= h_{ij}(t) \\
\frac{\partial J_i(t)}{\partial \beta_{ijs}} &= n_i(t - s)
\end{align}

\section{Experiments}
\section{Data Source \& Simulation}

We will test our model on actual multiple neuronal spike train data as well as on simulated spike trains. The actual data have been provided by the Buszaki Lab and was recorded simultaneously from 87 prefrontal cortex neurons of a behaving rat over the course of roughly 40 minutes. This data has been pre-processed so that we begin with the spike trains which are equivalent to the values of $n_i(t)$.

We also use simulated neuronal spike train data generated using equations (1)-(3) in \citep{mishchencko2011} which are similar to our equations \eqref{J}, \eqref{h}, and \eqref{f} except that instead of the term $I_i(t)$, they have a term representing some linearly filtered external stimulus, $k_i \cdot S^{ext}(t)$.

\subsection{Testing Method}

We will ensure the quality of our model and inference techniques on both types of data. As there is not ground-truth for the connection weights for actual data, we will be looking for weight values that reveal network motifs that resemble those found in the graphical analysis of known neuronal circuits such as the scale-free, small world networks with certain two, three, and four cell motifs \citep{song2005,perin2011}.

In the case of simulated data, we have an interesting opportunity to test the quality of the indirect component of our model. We can simulate a large collection neurons and only use a subset as the observed neurons in our model. We can then compare our inferred indirect weights with the known direct weights of the simulated data.

\section{Expected Conclusions \& Results}
We propose to develop and test a model for inferring the direct and indirect functional connectivity of a neural microcircuit given discrete multiple neuronal spike train data. As ground truth data is limited in this domain, we will primarily measure performance using the same simulation processes and metrics used in the source papers for our model.  Specifically, we expect our model to accurately retrieve the connectivity parameters used to generate the simulated data. We go a step further by comparing our inferred parameters for indirect influences on the observed neurons to known chains of direct connectivity which pass through unobserved neurons.

Further, since we have access to actual lab spike train data, we will also compare our model against existing models on real data.  A potential metric for performance in this regime may come from the few experiments done, where dozens of cells are voltage clamped simultaneously, and functional connections are accurately teased out through direct measurements of post synaptic currents.  Also, we expect to validate the existence of graphical features commonly associated with neural circuitry.

\begin{small}
\bibliographystyle{plainnat}
\bibliography{refs} 
\end{small}
\end{document}
